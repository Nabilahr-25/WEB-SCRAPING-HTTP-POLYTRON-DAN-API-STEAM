# -*- coding: utf-8 -*-
"""Scrapping Polytron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MwcTCLA-MUPO2gXhGSbbs8OXbMzS7abT

**Kelas 2024A**

Rebriane Atitha G. (039)

Nabilah Hilmi R. (053)

Kekila Akmal N. (144)

# WEB SCRAPING HTTP
"""

import requests

page = requests.get('https://polytron.co.id/')
page

# get the html document
print(page.text)

"""## Web Scraping with HTTP Request

### HTML Parser using BeautifulSoup
"""

from bs4 import BeautifulSoup

import requests
page = requests.get('https://polytron.co.id/')
bs = BeautifulSoup(page.text, 'html.parser')
bs

"""### get tag in html

get title of page
"""

bs.title

bs.title.name

bs.title.string

"""### find and find_all

**HEAD**
"""

bs.find("head")

bs.find_all("head")

"""**PRODUCT TITLE**"""

bs.find('div', {'class':'custom-product-title'})

bs.find('div', {'class':'custom-product-title'}).text

bs.find_all('div', {'class':'custom-product-title'})

product_title = bs.find_all('div', {'class':'custom-product-title'})
for i in product_title:
  print(i.text) #16 jenis

"""**REGULAR PRICE**"""

bs.find('del', {'id':'regular-price'})

real_price = bs.find('del', {'id':'regular-price'})
for i in real_price:
  print(i.text)

real_price = bs.find_all('del', {'id':'regular-price'})
for i in real_price:
  print(i.text)

"""***DISCOUNT PRICE***"""

bs.find('span', {'id':'discount-amount'})

discount = bs.find('span', {'id':'discount-amount'})
for i in discount:
  print(i.text)

discount = bs.find_all('span', {'id':'discount-amount'})
for i in discount:
  print(i.text)

"""***DISCOUNT PRICE***"""

search_price = bs.find_all('strong')
reguler_price = None
for i in search_price:
    dis_price = i.find('span', {'class': 'woocommerce-Price-amount amount'})
    if dis_price:
        reguler_price = dis_price()
        break
print(reguler_price)

search_price = bs.find_all('strong')
reguler_price = None
for i in search_price:
    dis_price = i.find('span', {'class': 'woocommerce-Price-amount amount'})
    if dis_price:
        reguler_price = dis_price.text.strip()
        break
print(reguler_price)

dis_tags = bs.find_all('strong')
for i in dis_tags:
    dis_price = i.find('span', {'class':'woocommerce-Price-amount amount'})
    if dis_price:
        print(dis_price.text)

"""### Banyak pages

Algorithm:

1. tentukan alamat url yang akan di store (tanpa page number)
2. buat array kosong sesuai dengan jumlah feature yang akan dimasukkan
3. buat loop dari page yang akan diambil
4. append terhadap array dari setiap tag
"""

url = 'https://polytron.co.id'
page = requests.get(url)
soup = BeautifulSoup(page.text, "html.parser")

product_title = []
reguler_price = []
discount = []
price_after_discount = []

title = soup.find_all('div', {'class':'custom-product-title'})
for p in title:
  product_title.append(p.text)

real_price = soup.find_all('del', {'id':'regular-price'})
for r in real_price:
  reguler_price.append(r.text)

discounts_price = bs.find_all('span', {'id':'discount-amount'})
for d in discounts_price:
  discount.append(d.text)

new_price = soup.find_all('strong')
for n in new_price:
  dis_price = n.find('span', {'class':'woocommerce-Price-amount amount'})
  if dis_price:
    price_after_discount.append(dis_price.text)

len(product_title), len(reguler_price), len(discount), len(price_after_discount)

import pandas as pd
df = pd.DataFrame(
    {'Jenis Barang':product_title,
     'Harga Sebelum Diskon':reguler_price,
     'Diskon':discount,
     'Harga terbaru':price_after_discount
     })
display(df)

"""# Chapter 4 - Text Pre-processing

remove HTML tags
"""

import re

def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)

text = """<div>
<h1>Produk Unggulan</h1>
<p>&nbsp</p>
<a href="https://polytron.co.id/register/">Click here to register</a>
<strong>Alasan Memilih TV Polytron</strong>
</div>"""

hasil_ht = remove_html(text)
print(hasil_ht)

"""Lower Casing"""

text = '''Dapatkan Penawaran Menarik di Store Kami.
Penawaran eksklusif hanya untuk Anda
PT Hartono Istana teknologi
Don't have an account? Register one!
'''

hasil_cf = str.lower(text)
print(hasil_cf)

"""Remove Punctuations"""

import string
punc = string.punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', punc))

hasil_rp = remove_punctuation(hasil_cf)
print(hasil_rp)

"""Remove Stopwords"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

print(stopwords.words('english'))
print(stopwords.words('indonesian'))

sw = set(stopwords.words('indonesian'))

def remove_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in sw])

hasil_st = remove_stopwords(hasil_rp)
print(hasil_st)

"""removal of frequent words"""

from collections import Counter
import re

def remove_frequent_words(texts, top_n=3):
    # 1. Tokenisasi sederhana (hanya ambil huruf/angka)
    tokenized = [re.findall(r'\b\w+\b', doc.lower()) for doc in texts]

    # Buang token huruf tunggal
    tokenized = [[w for w in doc if len(w) > 1] for doc in tokenized]

    # 2. Hitung frekuensi kata
    all_tokens = [token for doc in tokenized for token in doc]
    freq = Counter(all_tokens)

    # 3. Ambil kata-kata paling sering
    most_common_words = set([w for w, _ in freq.most_common(top_n)])

    # 4. Buat teks baru tanpa frequent words
    cleaned_texts = []
    for doc in tokenized:
        filtered = [word for word in doc if word not in most_common_words]
        cleaned_texts.append(" ".join(filtered))

    return cleaned_texts, most_common_words

docs = hasil_rp.splitlines()

cleaned, frequent = remove_frequent_words(docs, top_n=3)

print("Frequent words yang dihapus:", frequent)
print("\nTeks setelah dihapus:")
for c in cleaned:
    print("-", c)